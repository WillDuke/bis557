---
title: "homework-2"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{homework-2}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(mc.cores = 2)
library(casl)
library(bis557)
```



##1. CASL 2.11 Problem #5

**Consider the simple regression model with only a scalar *x* and intercept:**
$$y = \beta_0 + \beta_1 * x$$
**Using the explicit formula for the inverse of a 2-by-2 matrix, write down the least squares estimators for $\hat{\beta_0}$ and $\hat{\beta_1}$.**

We can express the SLR model in matrix form, $Y = X\beta$, where each term represents the following matrices:
$$\mathbf{Y}_{n \times 1} = \left[\begin{array}
{ccc}
y_{1} \\
y_{2}  \\
...  \\
y_{n}  \\
\end{array}\right] \hspace{1cm}
\mathbf{X}_{n \times 2} = \left[\begin{array}
{ccc}
1 & x_{1} \\
1 & x_{2}  \\
...  & ...\\
1 & x_{n}  \\
\end{array}\right] \hspace{1cm}
\mathbf{\beta} = \left[\begin{array}
{ccc}
\beta_{0} \\
\beta_{1}  \\
\end{array}\right]
$$


For OLS, we can solve directly for the $\beta$s with the following formula:
$$\beta = (X'X)^{-1}X'Y$$
Writing it out explicitly:
$$X'X = \mathbf{X}_{n \times 2} = \left[\begin{array}
{ccc}
n & \sum x_i \\
\sum x_i & \sum x_i^2  \\
\end{array}\right]$$

Now we take the inverse of the above 2x2 matrix using the explicit formula:

$$A = \left[\begin{array}
{ccc}
a & b \\
c & d  \\
\end{array}\right] \hspace{1cm} A' = \frac{1}{ad-bc}\left[\begin{array}
{ccc}
d & -b \\
-c & a  \\
\end{array}\right] $$

We find that the inverse of $X'X$ is:

$$(X'X)^{-1} = \frac{1}{n \sum x_i^2 - (\sum x_i)^2}\left[\begin{array}
{ccc}
\sum x_i)^2 & -\sum x_i) \\
-\sum x_i) & n  \\
\end{array}\right]$$
And we can see that the remaining cross product returns the following:
$$X'Y = \left[\begin{array}
{ccc}
1 & 1 & ... & 1 \\
x_1 & x_2 & ... & x_n  \\
\end{array}\right] \left[\begin{array}
{ccc}
y_1 \\
y_2 \\
... \\
y_n \\
\end{array}\right]=\left[\begin{array}
{ccc}
\sum y_i \\
\sum x_i y_i  \\
\end{array}\right]$$

Combining combining the two:

$$\beta = \frac{1}{n \sum x_i^2 - (\sum x_i)^2}\left[\begin{array}
{ccc}
\sum x_i^2 & -\sum x_i) \\
-\sum x_i & n  \\
\end{array}\right]\left[\begin{array}
{ccc}
\sum y_i \\
\sum x_i y_i  \\
\end{array}\right]$$
$$= \frac{1}{n \sum x_i^2 - (\sum x_i)^2}\left[\begin{array}
{ccc}
\sum x_i^2 \sum y_i - \sum x_i \sum x_i y_i \\
-\sum x_i \sum y_i + n \sum x_i y_i  \\
\end{array}\right]$$

Here, we find the explicit formula for $\beta$ for the given simple linear regression model.

##2. Ridge Regression with Collinearity Handling 

The function `ridge_regression` returns a list of $\beta$ coefficients while handling collinearity. 

```{r}
data("iris")
iris$duplicate <- iris$Sepal.Width
ridge_regression(Sepal.Length ~ ., iris, lambda = 0)
```

##3. Cross Validation for `ridge_regression`

The function `cv_ridge_regression` returns a list containing a tibble detailing the RMSEs, standard error of the means, and CI bounds; a plot of the lambda values tested and their corresponding RMSEs; as well as the lambda with the lowest RMSE, `lambda_min`, and the lambda 1 standard error higher, `lambda_1se`.

```{r}
data("cv_test")
cv_ridge_regression(response ~ ., cv_test)
```


##4.Numerical Stability, Statistical Error, and Ridge Regression
**Section 2.8 of CASL shows that as the numerical stability decreases, statistical errors increase. Reproduce
the results and then show that using ridge regression can increase numerical stability and decrease
statistical error.**

The book provides a simulation which shows that when numerical stability decreases (in this case, when the design matrix has linearly dependent columns), the error in the estimate of beta increases.

The following code returns the condition number of a $1000 \times 25$ matrix of random values by taking the ratio of the max and min singular values. Simulating the error rate for the OLS beta estimate reveals that when the condition number is low, the error in estimating beta tends to be low as well.

```{r}
n <- 1000; p <- 25
beta <- c(1, rep(0, p-1))
X  <- matrix(rnorm(n * p), ncol = p)
svals <- svd(X)$d
max(svals)/min(svals)

N <- 1e4; `12_errors` <- rep(0, N)
for (k in 1:N){
  y <- X %*% beta + rnorm(n)
  betahat <- casl_ols_svd(X, y)
  `12_errors`[k] <- sqrt(sum((betahat - beta)^2))
}
mean(`12_errors`)
```

In contrast, modifying X so that one of its columns is a linear combination of one of the others dramatically increases the condition number and the corresponding error in the $\beta$ estimate.

```{r}
n <- 250; p <- 25
beta <- c(1, rep(0, p-1))
X  <- matrix(rnorm(n * p), ncol = p)
alpha <- 0.001
X[,1] <- X[,1] * alpha + X[,2] * (1 - alpha)
svals <- svd(X)$d
max(svals) / min(svals)
```


```{r}
N <- 1e4; `12_errors` <- rep(0, N)
for (k in 1:N){
  y <- X %*% beta + rnorm(n)
  betahat <- solve(crossprod(X), crossprod(X,y))
  `12_errors`[k] <- sqrt(sum((betahat - beta)^2))
}
mean(`12_errors`)
```

Notably, the error in the estimate is substantially lower than the condition number, because (as the book describes) the condition number is a "worst case scenario", and the estimate is usually much better than that.


We now want to show that we can increase numerical stability and improve the accuracy of our estimate using ridge regression. Taking the gradient of the ridge regression formula and solving directly for beta and the condition number, we can find that the new matrix we will inverse is: $X'X + 1_p \cdot \lambda$, and the equivalent condition number to above is: $\frac{\sigma_{max} + \lambda}{\sigma_{min} + \lambda}$. With this method, we can avoid dividing by a very small (or even zero) $\sigma_{min}$, thereby substantially reducing the condition number.

As you can see below, adding a $\lambda$ of 0.13 reduces the condition number to a fraction of its original value.

```{r}
lambda <- 0.3293489
svals <- svd(X)$d
(max(svals) +  lambda) / (min(svals) + lambda)
```

By simulating the error of the estimate calculated directly with the lambda term, we can see that there is a corresponding (albeit small, in this case) reduction in the variance. By introducing some bias, we can reduce the statistical error of our estimate.

```{r}
N <- 1e4; `12_errors` <- rep(0, N)
for (k in 1:N){
  y <- X %*% beta + rnorm(n)
  betahat <- solve(crossprod(X) + lambda, crossprod(X,y))
  `12_errors`[k] <- sqrt(sum((betahat - beta)^2))
}
mean(`12_errors`)
```
##5. The LASSO Penalty

**Consider the LASSO penalty:**
$$\frac{1}{2n} ||Y - X \beta||^2_2 + \lambda ||\beta||_1$$
**Show that if $|X_j^TY| \le n \lambda$, then $\hat{\beta}^{LASSO}$ must be zero.**

The general formula for an estimated beta vector from the coordinate descent algorithm for solving the elastic net equation is:

$$\tilde{\beta}_l = \frac{\frac{1}{n} \sum^n_{i=1} x_{il} (y_i -\tilde{y}^{(l)}) - \lambda \alpha}{1 - \lambda (1 - \alpha)}$$
Setting $\alpha$ to 1 for ridge regression:

$$\tilde{\beta}_l = \frac{1}{n} \sum^n_{i=1} x_{il} (y_i -\tilde{y}^{(l)}) - \lambda$$

We can re-express this in matrix notation as:

$$\tilde{\beta}_l = \frac{1}{n} X^t_j Y - \lambda$$

We know that if the function returns a negative or zero $\beta$, $\beta$ will be set to zero (a soft threshold). Since $\beta$ is either squared or within an absolute value in the objective function, the result is the same regardless of the sign of $\beta$: We can see from the equation above that if $|X^t_j Y| \le n \lambda$, then $\tilde{\beta}_l$ will be negative (and thus set to zero).
